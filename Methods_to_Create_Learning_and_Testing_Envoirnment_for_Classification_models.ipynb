{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6wUwA_BM_64"
      },
      "source": [
        "# Methods to Create Learning and Testing Envoirnment for Classification models\n",
        "\n",
        "Broadly, classificaition models works in majorly two steps namely, learning and testing. Where the learning phase is used to train the classification model whereas, performance of the trained model is evaluated during testing phase. In order to design these steps, the given data set is divided into training and test sets. The key difference between these two subsets of data set is that the information on class labels is provided to the model whereas, in test set the class labels are hidden from the model. \n",
        "\n",
        "There are different techniques that can use the given data set to create robust learning the and create useful estimates of performance of the classification models. Below are useful techniques\n",
        "\n",
        "1. Hold-out\n",
        "\n",
        "Under Hold-out method, given data set is randomly divided into training set(70-80%) and test set (30-20%). The training data set is used to train the model and the trained model is tested is on test data set using evaluation metrices such as accuracy, confusion matrix, precision, recall and more. \n",
        "\n",
        "Key features of  Hold-out technique\n",
        "\n",
        "1. It is the simplest method to train and test any classification model. The size of split \n",
        "can depend on the size and specifics of the data set. Although (70-30)% is common split \n",
        "setting.\n",
        "\n",
        "2. Using Hold-out, the algorithm evaluation is fast. \n",
        "\n",
        "3. It is suitable for large data sets where there is strong evidence that both the slipts \n",
        "of the data are true representative of the underlying domain.\n",
        "\n",
        "4. The major downside of Hold-out method is that the model trained can be a high variance \n",
        "model. It is for the reason that model training is done on only one sample of training \n",
        "data. This kind of learning where only one sample is given to the model to train may \n",
        "not develop a robut learning. A robust/ low variance model do not fluctuate much in \n",
        "performance whereas, high variance model indicates less stability in terms of \n",
        "performance. This method does not allow a exhaustive learning of the model. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Repeated Hold-out\n",
        "\n",
        "This is the variation of Hold-out method. Where instead of making one sample set of training and test sets, we create n sample sets of traning and test sets. Where n is entered by the user. The model is trained and tested on each sample set and performance is evaluated. This way, we get performance of the model on n different sample inputs. The average performance of the model can be evaluated by taking average of performances on different runs. \n",
        "\n",
        "Key features of  Repeated Hold-out technique\n",
        "\n",
        "1. It is suitable for large data sets.\n",
        "\n",
        "2. Repeated Hold-out helps to produce low variance model( because of differnt n training \n",
        "samples are used for learning).\n",
        "\n",
        "3.  The major downside of this method is that creation of n samples of training and test \n",
        "sets from the same data set may produce repititions in train and test splits. \n",
        "\n",
        "\n",
        "3. k-fold cross validation\n",
        "\n",
        "k-fold cross validation is an approach helps to develop more robust/stable and low variance model. Its key strategy is that we first create random set of training and test set for the given dataset. Apply k-fold cross validation on training set. Where given training set is divided into k folds. The model is trained on k-1 folds and one fold held back for evaluation. This process is repeated so that each fold is given a chance once as training and once as testing. After running k-cross validation, we end up with k different performance scores that we can summerize using mean. The mean score gives us the estimate of performance of model on the unseen data. Under K-fold cross validation, model undergoes rigorous training by providing it different set of training data on each run. The mean performance of the model more than 80% under K-fold cross validation ensures low varianace and highly stable model. \n",
        "\n",
        "Once we get estimate of stability of the model, we develop and test the model using the splits created in the initial step of the technique. \n",
        "\n",
        "Key features of  k-fold cross validation\n",
        "\n",
        "1. This method is the more reliable estimate of the performance of the model on new data. It is more accurate because the algorithm is trained and evaluated mulitple times on different data. \n",
        "    \n",
        "2. The choice of k  is usually determined by the number of instances contained in our dataset. For eg., suppose given train data set has n=15 instances. Let k is set to 2. It means that given data set will be divided into two equally sized sets. In this case, set 1 containing 8 samples(53% of n) of data and set 2 with 7 samples(46% of n). Under 2-fold cross validation, set 1( 53% ) of data will be used once for training and once for testing. Similarly set 2(46%) of data will be used once for training and once for testing. At the end, mean performance is evaluated for both the sets. Interestingly, under this setting, we are giving model equal amount of data for learning and testig. Ideally, we train model on more data so that the learning becomes strong. Setting k =2 with data set of instances 15 may not be a good choice. Lets set k =3. We get three equally distributed sets for n =15. Under each run of 3 fold cross validation, 2 out of 3 sets (which means 66% of the data) will be used for training and remaining 1 set (which means 33% of the data) for testing. This setting is much better than the choice of setting k =1 because here we are giving more data to the model to train. \n",
        "    \n",
        "3. For modest sized data sets in thousands, k value 5,10 are most common.\n",
        "    \n",
        "\n",
        "\n",
        "4. Leave One Out Cross Validation\n",
        "\n",
        "It is very similar to k-fold cross validation with once key difference that here k is set to number of observations in the data set. The key features of this method are same as k-fold cross valiation. Howerver, one downside of this method is that it can be computationally more expensive procedure than k-fold cross validation. It is good choice when data set size is small and aim is to balance model variance\n",
        "\n",
        "\n",
        "This Python hands-on session will demonstrate implementation of these techniques on  heart data set. The data set is taken from https://www.kaggle.com/ronitf/heart-disease-uci. The size of the data set is (303 x 14). The tutorial will help you learn how to train and test classification model under four methods described above. The classification model considered for illustration is Decision tree. However, the four methods of defining train and test enviornment defined above remains the same for any other classification model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU7vQEE7M_7D"
      },
      "source": [
        "# 1. Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41n9Kjr0M_7E"
      },
      "outputs": [],
      "source": [
        "# importing Pandas for data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# importing decision tree classifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# importing method for Hold-out\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# importing method for k-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# importing method for evaluating performance score of the model under k-fold cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# importing method for repeated Hold-out\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# importing method for leave one out cross validation\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "\n",
        "#importing methods for model evaluation\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn7SpBwYM_7H"
      },
      "source": [
        "# 2. Loading data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3xyK2mEM_7J",
        "outputId": "774fd6b2-bf1d-4772-8a1c-44d221145053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            "age         303 non-null int64\n",
            "sex         303 non-null int64\n",
            "cp          303 non-null int64\n",
            "trestbps    303 non-null int64\n",
            "chol        303 non-null int64\n",
            "fbs         303 non-null int64\n",
            "restecg     303 non-null int64\n",
            "thalach     303 non-null int64\n",
            "exang       303 non-null int64\n",
            "oldpeak     303 non-null float64\n",
            "slope       303 non-null int64\n",
            "ca          303 non-null int64\n",
            "thal        303 non-null int64\n",
            "target      303 non-null int64\n",
            "dtypes: float64(1), int64(13)\n",
            "memory usage: 33.2 KB\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_csv(\"Data sets/heart.csv\")\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjXjTHEHM_7K"
      },
      "source": [
        "# 3. Creating Training and Test split\n",
        "\n",
        "This step remains common for Hold-out, Repeated Hold-out, k-cross validationa and Leave one out cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayFufYVjM_7L",
        "outputId": "ac7fadd8-d31a-4797-cfd3-ad1ad82750a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sample training data without target feature\n",
            "\n",
            "(212, 13)\n",
            "\n",
            "The sample with only target feature\n",
            "\n",
            "(91,)\n"
          ]
        }
      ],
      "source": [
        "# My_data contains all data points from My_data set from from first feature to 12th feature(indicator features)\n",
        "My_data = dataset.iloc[:,0:13] \n",
        "\n",
        "# My_target contains class information which is 13th feature in the data set of \n",
        "\n",
        "My_data_target=dataset.iloc[:,13]\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(My_data, My_data_target, test_size=0.3, random_state=10)\n",
        "\n",
        "print(\"The sample training data without target feature\\n\")\n",
        "print(X_train.shape)\n",
        "print(\"\\nThe sample with only target feature\\n\")\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPXeoKUqM_7M"
      },
      "source": [
        "# 4. Learning and Testing Model Performance under Hold-out method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UMDZMgoM_7M",
        "outputId": "1c16c56f-062e-4463-d353-1a7bdc839072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 78.0 %\n",
            "---------------\n",
            "Confusion matrix\n",
            "---------------\n",
            "[[34 16]\n",
            " [ 4 37]]\n",
            "---------------\n",
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.89      0.68      0.77        50\n",
            "     class 1       0.70      0.90      0.79        41\n",
            "\n",
            "    accuracy                           0.78        91\n",
            "   macro avg       0.80      0.79      0.78        91\n",
            "weighted avg       0.81      0.78      0.78        91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# creating instance of Decision tree classifier\n",
        "\n",
        "DT_model_Holdout = DecisionTreeClassifier()\n",
        "\n",
        "# fitting the model to training data set\n",
        "DT_model_Holdout.fit(X_train, Y_train)\n",
        "\n",
        "# Getting prediction on test set\n",
        "\n",
        "DT_model_Holdout_pred_test= DT_model_Holdout.predict(X_test)\n",
        "\n",
        "# Computing Model Accuracy\n",
        "\n",
        "print(\"Accuracy:\",round(metrics.accuracy_score(Y_test, DT_model_Holdout_pred_test),2) * 100, \"%\")\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "# Printing confusion matrix\n",
        "\n",
        "print (\"Confusion matrix\")\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "print(metrics.confusion_matrix(Y_test, DT_model_Holdout_pred_test))\n",
        "\n",
        "# Model detailed classification report\n",
        "target_names = ['class 0', 'class 1']\n",
        "\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "print(\"Classification report\", metrics.classification_report(Y_test, DT_model_Holdout_pred_test,target_names =target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Voz6Vja0M_7N"
      },
      "source": [
        "The model accuracy on test set is 78%(which appears to be good). However, model being trained on Hold-out method can not ensure its stability in performance on different test sets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSt4lrs2M_7N"
      },
      "source": [
        "# 5. Learning and Testing Model Performance under Repeated Hold-out method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW8kDgEZM_7N",
        "outputId": "97773166-ed0a-4cd2-bd35-9e248eeddf96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      score\n",
            "0  0.765625\n",
            "1  0.828125\n",
            "2  0.765625\n",
            "3  0.796875\n",
            "4  0.796875\n",
            "5  0.703125\n",
            "6  0.671875\n",
            "7  0.828125\n",
            "8  0.687500\n",
            "9  0.734375\n",
            "The mean performance of the model using Repeated Hold out: \n",
            "\n",
            "Average:  score    76.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# creating instance of Decision tree classifier\n",
        "DT_Repeated_holdout = DecisionTreeClassifier()\n",
        "# Creating instance of required number of random splits and sizes of training and test sets.\n",
        "# following code will create 10 random training and test sets with ratio 70-30% ratio.\n",
        "Repeated_holdout = ShuffleSplit(n_splits = 10, test_size=.30,random_state=10)\n",
        "# Evaluating performance of model on each sample set of training and test set\n",
        "Repeated_holdout_results = cross_val_score(DT_Repeated_holdout,\n",
        "                                           X_train,Y_train, cv= Repeated_holdout \n",
        "                                          )\n",
        "Model_Eval_Score_Repeatedholdout =[]\n",
        "Model_Eval_Score_Repeatedholdout.append(Repeated_holdout_results)\n",
        "CV_IterationsBy_Repeatedholdout = pd.DataFrame(np.transpose(Model_Eval_Score_Repeatedholdout), columns=['score'])\n",
        "print(CV_IterationsBy_Repeatedholdout)\n",
        "\n",
        "# printing the mean of accuracy the model\n",
        "print(\"The mean performance of the model using Repeated Hold out: \\n\")\n",
        "print(\"Average: \", round(CV_IterationsBy_Repeatedholdout.mean(), 2)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCz9utauM_7O"
      },
      "source": [
        "The model performance at each iteration is outputted. The average accuracy of 73% indicates models stability when experimented on 10 unseen data. Once we accept this stability then next step is to  do the final training of the model and test it for given test set as done in Hold-out method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5WBqlGDM_7O"
      },
      "source": [
        "# 6. Testing Model Performance under Repeated Hold-out method\n",
        "\n",
        "This step remains the same in all methods. Where model learning and testing is performed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWk2YNsJM_7P",
        "outputId": "2b0ebd8c-753e-4f52-dd18-fa8edffe8178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 77.0 %\n",
            "---------------\n",
            "Confusion matrix\n",
            "---------------\n",
            "[[33 17]\n",
            " [ 4 37]]\n",
            "---------------\n",
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.89      0.66      0.76        50\n",
            "     class 1       0.69      0.90      0.78        41\n",
            "\n",
            "    accuracy                           0.77        91\n",
            "   macro avg       0.79      0.78      0.77        91\n",
            "weighted avg       0.80      0.77      0.77        91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DT_Repeated_holdout.fit(X_train, Y_train)\n",
        "\n",
        "# Getting prediction on train and test sets\n",
        "\n",
        "DT_model_RepeatedHoldout_pred_test= DT_Repeated_holdout.predict(X_test)\n",
        "\n",
        "# Computing Model Accuracy\n",
        "\n",
        "print(\"Accuracy:\",round(metrics.accuracy_score(Y_test, DT_model_RepeatedHoldout_pred_test),2) * 100, \"%\")\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "# Printing confusion matrix\n",
        "\n",
        "print (\"Confusion matrix\")\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "print(metrics.confusion_matrix(Y_test,DT_model_RepeatedHoldout_pred_test))\n",
        "\n",
        "# Model detailed classification report\n",
        "target_names = ['class 0', 'class 1']\n",
        "\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "print(\"Classification report\", metrics.classification_report(Y_test, DT_model_RepeatedHoldout_pred_test,target_names =target_names))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYZGowzEM_7P"
      },
      "source": [
        "The output is same as that of Hold -out method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXVFDpnUM_7P"
      },
      "source": [
        "# 7. Learning and Testing Model Performance under k-cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJuqvmB0M_7Q",
        "outputId": "0b919ec6-d778-47fc-8e95-90e47384c66b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      score\n",
            "0  0.681818\n",
            "1  0.590909\n",
            "2  0.619048\n",
            "3  0.714286\n",
            "4  0.809524\n",
            "5  0.761905\n",
            "6  0.761905\n",
            "7  0.619048\n",
            "8  0.666667\n",
            "9  0.761905\n",
            "The mean performance of the model using Repeated Hold out: \n",
            "\n",
            "Average:  score    70.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# creating instance of decision tree classifier\n",
        "DT_Kfold = DecisionTreeClassifier()\n",
        "# Setting number of folds as 10\n",
        "Kfold = KFold(n_splits = 10,random_state=10)\n",
        "#evaluating model performance on each fold\n",
        "Repeated_holdout_results = cross_val_score(DT_Kfold,X_train,Y_train, cv= Kfold \n",
        "                                          )\n",
        "Model_Eval_Score_kfold =[]\n",
        "Model_Eval_Score_kfold.append(Repeated_holdout_results)\n",
        "CV_IterationsBy_model_kfold = pd.DataFrame(np.transpose(Model_Eval_Score_kfold), columns=['score'])\n",
        "print(CV_IterationsBy_model_kfold)\n",
        "\n",
        "# printing the mean of accuracy of each model\n",
        "print(\"The mean performance of the model using Repeated Hold out: \\n\")\n",
        "print(\"Average: \", round(CV_IterationsBy_model_kfold.mean(), 2)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqe7ZXb5M_7R"
      },
      "source": [
        "The model performance at each iteration is outputted. The average accuracy of 71% indicates models stability when experimented on 10 unseen data. The difference in average performance by repeated hold-out and k fold cross validation is due to the fact that in the repeated hold-out there are repitations of instances but, in k cross-validation such repitations does not exist. \n",
        "\n",
        "Once we accept this stability then next step is to  do the final training of the model and test it for given test set as done in Hold-out method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Krbn0HM_7R"
      },
      "source": [
        "# 8. Testing Model Performance under k-fold Cross Validation\n",
        "\n",
        "This step remains the same in all methods. Where model learning and testing is performed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoeho8NWM_7R",
        "outputId": "3325f89e-af84-4e4f-a849-1f4410eeb860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 77.0 %\n",
            "---------------\n",
            "Confusion matrix\n",
            "---------------\n",
            "[[33 17]\n",
            " [ 4 37]]\n",
            "---------------\n",
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.89      0.66      0.76        50\n",
            "     class 1       0.69      0.90      0.78        41\n",
            "\n",
            "    accuracy                           0.77        91\n",
            "   macro avg       0.79      0.78      0.77        91\n",
            "weighted avg       0.80      0.77      0.77        91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DT_Kfold.fit(X_train, Y_train)\n",
        "\n",
        "# Getting prediction on train and test sets\n",
        "\n",
        "DT_model_kfold_pred_test= DT_Kfold.predict(X_test)\n",
        "\n",
        "# Computing Model Accuracy\n",
        "\n",
        "print(\"Accuracy:\",round(metrics.accuracy_score(Y_test, DT_model_kfold_pred_test),2) * 100, \"%\")\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "# Printing confusion matrix\n",
        "\n",
        "print (\"Confusion matrix\")\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "print(metrics.confusion_matrix(Y_test,DT_model_kfold_pred_test))\n",
        "\n",
        "# Model detailed classification report\n",
        "target_names = ['class 0', 'class 1']\n",
        "\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "print(\"Classification report\", metrics.classification_report(Y_test, DT_model_kfold_pred_test,target_names =target_names))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KY2mQmkM_7U"
      },
      "source": [
        "The output is same as that of Hold -out method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y_ipiE0M_7W"
      },
      "source": [
        "# 9. Learning and Testing Model Performance under Leave One Out Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtFF960_M_7W",
        "outputId": "2af54578-4674-47dc-8d6c-149b3711e4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The mean performance of the model using Repeated Hold out: \n",
            "\n",
            "Average:  score    73.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# creating instance of decision tree classifier\n",
        "DT_LOOCV = DecisionTreeClassifier()\n",
        "loocv= LeaveOneOut()\n",
        "# evaluating model performance on each fold\n",
        "LOOCV_results = cross_val_score(DT_LOOCV,X_train,Y_train, cv= loocv\n",
        "                                          )\n",
        "Model_Eval_Score_LOOCV =[]\n",
        "Model_Eval_Score_LOOCV.append(LOOCV_results)\n",
        "CV_IterationsBy_model_LOOCV = pd.DataFrame(np.transpose(Model_Eval_Score_LOOCV), columns=['score'])\n",
        "#print(CV_IterationsBy_model_LOOCV)\n",
        "\n",
        "# printing the mean of accuracy of each model\n",
        "print(\"The mean performance of the model using Repeated Hold out: \\n\")\n",
        "print(\"Average: \", round(CV_IterationsBy_model_LOOCV.mean(), 2)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRnbD0PYM_7X"
      },
      "source": [
        "# 10. Testing Model Performance under k-fold Cross Validation\n",
        "This step remains the same in all methods. Where model learning and testing is performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxLZhwdDM_7Y",
        "outputId": "35a13b9f-fbb5-4a88-c610-48ea72b6c6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 74.0 %\n",
            "---------------\n",
            "Confusion matrix\n",
            "---------------\n",
            "[[31 19]\n",
            " [ 5 36]]\n",
            "---------------\n",
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.86      0.62      0.72        50\n",
            "     class 1       0.65      0.88      0.75        41\n",
            "\n",
            "    accuracy                           0.74        91\n",
            "   macro avg       0.76      0.75      0.74        91\n",
            "weighted avg       0.77      0.74      0.73        91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DT_LOOCV.fit(X_train, Y_train)\n",
        "\n",
        "# Getting prediction on train and test sets\n",
        "\n",
        "DT_model_LOOCV_pred_test= DT_LOOCV.predict(X_test)\n",
        "\n",
        "# Computing Model Accuracy\n",
        "\n",
        "print(\"Accuracy:\",round(metrics.accuracy_score(Y_test, DT_model_LOOCV_pred_test),2) * 100, \"%\")\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "# Printing confusion matrix\n",
        "\n",
        "print (\"Confusion matrix\")\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "print(metrics.confusion_matrix(Y_test,DT_model_LOOCV_pred_test))\n",
        "\n",
        "# Model detailed classification report\n",
        "target_names = ['class 0', 'class 1']\n",
        "\n",
        "\n",
        "print (\"---------------\")\n",
        "\n",
        "print(\"Classification report\", metrics.classification_report(Y_test, DT_model_LOOCV_pred_test,target_names =target_names))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fek6l1XHM_7Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}